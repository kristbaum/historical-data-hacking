{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ea027b",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 – Daten-Preprocessing\n",
    "\n",
    "Ziel: Rohdaten aus `buildings.csv` explorieren, bereinigen, normalisieren und für weitere Schritte (Anreicherung / Visualisierung) vorbereiten.\n",
    "\n",
    "Hinweis: Bei der Verwendung der [Wikimedia PAWS Instanz](https://hub-paws.wmcloud.org/hub), bitte dieses Notebook und die zugehörigen Dateien extra hochladen (mit dem kleinen Uploadpfeil in der Seitenleiste). (Github Link)[https://github.com/kristbaum/historical-data-hacking] zum Download der Zip-Datei.\n",
    "\n",
    "## Installation von Abhängigkeiten\n",
    "\n",
    "* [Pandas](https://pandas.pydata.org/) \n",
    "* [Numpy](https://numpy.org/)\n",
    "* [Shapely](https://shapely.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b67db3-48a4-4d99-a102-88506cc0abdd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages into this notebook's Python kernel\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q pandas numpy shapely  # shapely for geospatial stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24d377-a3fa-4682-ba8f-82fed59cec54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Viele Spalten enthalten gemischte / freie Werte\n",
    "# → zunächst als Strings importieren vermeidez fehlerhafte automatische Typkonvertierungen.\n",
    "buildings = pd.read_csv('../buildings.csv', dtype=str)\n",
    "print(buildings.shape)\n",
    "\n",
    "# Zeigt einen Ausschnitt der ersten 5 Zeilen der Tabelle\n",
    "buildings.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b0bb3-877b-440d-8196-445904f7f175",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Erste Profilierung\n",
    "\n",
    "1. Zähle Null/Leer-Werte pro Spalte (`''`, `' '`, `NaN`).\n",
    "2. Ermittle Anteil gefüllter Werte für Kernfelder: `ID`, `appellation`, `verbaleDating`, `locationLat`, `locationLng`.\n",
    "3. Erzeuge eine Kurztabelle (DataFrame) mit (Spaltenname, Non-Null %, Anzahl unterschiedlicher Werte, Beispielwerte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54306dd-f123-442c-ba03-c683ed24883c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kopiere Rohdata für Profilierung\n",
    "raw_data = buildings.copy()\n",
    "\n",
    "# Ersetze Leerstrings und Spaces durch NA (fehlende Werte)\n",
    "data_with_normalized_nulls = raw_data.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Liste für Spaltenprofile erstellen\n",
    "profile_list = []\n",
    "\n",
    "# Iteriere über alle Spalten und sammle Statistiken\n",
    "for column_name in data_with_normalized_nulls.columns:\n",
    "    column_series = data_with_normalized_nulls[column_name]\n",
    "    \n",
    "    # Berechne Anteil nicht-Null Werte (in Prozent)\n",
    "    non_null_percentage = column_series.notna().mean() * 100\n",
    "    \n",
    "    # Zähle einzigartige Werte (ohne NA)\n",
    "    unique_value_count = column_series.nunique(dropna=True)\n",
    "    \n",
    "    # Sammle erste 3 Beispielwerte\n",
    "    sample_values_str = ', '.join(column_series.dropna().unique()[:3])\n",
    "    \n",
    "    # Füge Profil zur Liste hinzu\n",
    "    profile_list.append({\n",
    "        'column': column_name,\n",
    "        'non_null_pct': non_null_percentage,\n",
    "        'n_unique': unique_value_count,\n",
    "        'sample_values': sample_values_str\n",
    "    })\n",
    "\n",
    "# Erstelle DataFrame aus den Profilen und sortiere nach Anteil nicht-Null Werte\n",
    "profile_df = pd.DataFrame(profile_list).sort_values('non_null_pct')\n",
    "profile_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Entferne Spalten mit sehr wenigen/nicht vorhandenen Werten\n",
    "# Schwelle in Prozent (anpassbar): Spalten mit weniger als diesem Anteil an nicht-leeren Werten werden entfernt\n",
    "min_non_null_pct = 5.0  # z. B. 5%\n",
    "\n",
    "# Behandle '' und ' ' als fehlend für die Berechnung (ohne Side-Effects auf buildings)\n",
    "normalized = buildings.replace({'': pd.NA, ' ': pd.NA}).infer_objects(copy=False)\n",
    "non_null_pct = normalized.notna().mean() * 100\n",
    "\n",
    "# Optional: wichtige Spalten schützen, damit sie nicht versehentlich entfernt werden\n",
    "protected_cols = [c for c in ['ID', 'appellation', 'verbaleDating', 'locationLat', 'locationLng'] if c in buildings.columns]\n",
    "\n",
    "# Bestimme zu entfernende Spalten\n",
    "_to_drop = non_null_pct[non_null_pct < min_non_null_pct].index.tolist()\n",
    "# geschützte Spalten nicht droppen\n",
    "_to_drop = [c for c in _to_drop if c not in protected_cols]\n",
    "\n",
    "# Entferne Spalten in-place\n",
    "before_cols = buildings.shape[1]\n",
    "buildings.drop(columns=_to_drop, inplace=True, errors='ignore')\n",
    "after_cols = buildings.shape[1]\n",
    "\n",
    "print(f\"Entfernte Spalten (non-null < {min_non_null_pct}%): {len(_to_drop)} | Vorher: {before_cols}, Nachher: {after_cols}\")\n",
    "if _to_drop:\n",
    "    print('Beispiele:', ', '.join(_to_drop[:10]) + (' ...' if len(_to_drop) > 10 else ''))\n",
    "\n",
    "# Optional: zeige die 10 leersten verbleibenden Spalten zur Kontrolle\n",
    "print('\\nLeerste verbleibende Spalten (Top 10):')\n",
    "print(non_null_pct.loc[buildings.columns].sort_values().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4defa",
   "metadata": {},
   "source": [
    "Fragen:\n",
    "\n",
    "- Welche Spalten sind fast leer und sollten ggf. ausgelagert / ignoriert werden?\n",
    "- Gibt es offensichtliche Duplikate bei `ID`?\n",
    "\n",
    "## 2. Bereinigung `verbaleDating`\n",
    "\n",
    "Spalte enthält verbale Zeitangaben / Bereiche wie: `\"1000-2020, 1773-1776\"`.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "1. Zerlege `verbaleDating` in einzelne Segmente (Trennzeichen: Komma) → normalisiere Leerzeichen.\n",
    "2. Identifiziere Bereiche (Pattern `YYYY-YYYY`) vs. Einzeljahre (`YYYY`).\n",
    "3. Baue eine normalisierte Tabelle `building_dates`: (`building_id`, `date_raw`, `year_start`, `year_end`, `is_range`, `precision`).\n",
    "4. Berechne je Gebäude: minimaler Start, maximaler Endwert → `chronology_min`, `chronology_max` und füge sie wieder dem Haupt-DataFrame hinzu.\n",
    "5. Detektiere Ausreißer (z. B. Jahr < 1000 oder > aktuelles Jahr). Markiere sie für manuelle Prüfung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1345e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rows = []\n",
    "for _, r in buildings[['ID','verbaleDating']].fillna('').iterrows():\n",
    "    parts = [p.strip() for p in r.verbaleDating.split(',') if p.strip()]\n",
    "    for p in parts:\n",
    "        m_range = re.fullmatch(r'(\\d{4})-(\\d{4})', p)\n",
    "        m_year  = re.fullmatch(r'(\\d{4})', p)\n",
    "        if m_range:\n",
    "            y1, y2 = map(int, m_range.groups())\n",
    "            rows.append((r.ID, p, y1, y2, True, 'year-range'))\n",
    "        elif m_year:\n",
    "            y = int(m_year.group(1))\n",
    "            rows.append((r.ID, p, y, y, False, 'year'))\n",
    "        else:\n",
    "            # Nicht-parsbare Fälle separat behalten\n",
    "            rows.append((r.ID, p, None, None, None, 'unparsed'))\n",
    "\n",
    "building_dates = pd.DataFrame(rows, columns=['building_id','date_raw','year_start','year_end','is_range','precision'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac69222",
   "metadata": {},
   "source": [
    "Optionale Normalisierung unparsed Tokens: RegEx erweitern (z. B. `ca. 1750`, `17. Jh.` → Mapping Tabellen). Dokumentiere Annahmen!\n",
    "\n",
    "Metriken:\n",
    "\n",
    "- Anteil parsebarer Segmente.\n",
    "- Häufigste unparsed Muster (Top 10).\n",
    "\n",
    "## 3. Geodaten-Validierung+Auswertung\n",
    "\n",
    "1. Konvertiere `locationLat` / `locationLng` zu Float; markiere Zeilen, bei denen das misslingt.\n",
    "2. Prüfe Wertebereiche (Lat ∈ [-90, 90], Lng ∈ [-180, 180]). \n",
    "3. a) Auf Hessen beschränken mit Geokoordinate ( eines gedachten Rechtecks um Hessen (einfach von einer Karte von Hand geschätzt))\n",
    "3. b) Auf das historische Hessen Nassau beschränken mit Geoshape. Daten von [OpenHistoricalMap](https://www.openhistoricalmap.org/relation/2690442) und dann [Abfrage mittels overpass](https://overpass-turbo.openhistoricalmap.org/?Q=%5Bout%3Ajson%5D%5Btimeout%3A120%5D%3B%0Arel%282690442%29%3B%0A%28._%3B%3E%3B%29%3B%0Aout%20body%20geom%3B&C=50.004209%3B11.518449%3B7) (Export Button, geoshape wählen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05462199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(s):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except (TypeError, ValueError):\n",
    "        return pd.NA\n",
    "\n",
    "coords = buildings[['locationLat','locationLng']].map(to_float)\n",
    "valid_lat = coords.locationLat.between(-90,90)\n",
    "valid_lng = coords.locationLng.between(-180,180)\n",
    "buildings['coord_valid'] = valid_lat & valid_lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a) Filter: Bounding Box für Hessen (vereinfachte Methode)\n",
    "# Bounding Box ungefähr für Hessen (Nordwest / Südost):\n",
    "NW_LAT, NW_LNG = 51.599, 8.072   # Nordwestlichster Punkt\n",
    "SE_LAT, SE_LNG = 49.495, 10.088  # Südöstlichster Punkt\n",
    "\n",
    "\n",
    "# Min/Max für Bereichsprüfung bestimmen\n",
    "min_lat, max_lat = SE_LAT, NW_LAT\n",
    "min_lng, max_lng = NW_LNG, SE_LNG\n",
    "\n",
    "# Masken für BBox prüfen (nur valide Koordinaten berücksichtigen)\n",
    "in_bbox = (\n",
    "    coords.locationLat.between(min_lat, max_lat)\n",
    "    & coords.locationLng.between(min_lng, max_lng)\n",
    ")\n",
    "mask_bbox = in_bbox & buildings['coord_valid'].fillna(False)\n",
    "\n",
    "# Ergebnis: Gefilterte Entitäten in Hessen (via BBox)\n",
    "buildings['in_hesse_bbox'] = mask_bbox\n",
    "buildings_hesse_bbox = buildings.loc[mask_bbox].copy()\n",
    "\n",
    "print(\n",
    "    f\"BBox Hessen: lat [{min_lat}, {max_lat}], lng [{min_lng}, {max_lng}] | \"\n",
    "    f\"Treffer: {mask_bbox.sum()} von {len(buildings)}\"\n",
    ")\n",
    "buildings_hesse_bbox.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad614f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b) Punkt-in-Polygon: Historisches Hessen-Nassau\n",
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "# GeoJSON laden (Datei im aktuellen Ordner erwartet)\n",
    "with open('./hessen_nassau.geojson', 'r', encoding='utf-8') as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "# Geometrie aus FeatureCollection/Feature/Geometry entnehmen\n",
    "geom = (\n",
    "    gj['features'][0]['geometry'] if gj.get('type') == 'FeatureCollection'\n",
    "    else (gj['geometry'] if gj.get('type') == 'Feature' else gj)\n",
    ")\n",
    "hesse_nassau_polygon = shape(geom)\n",
    "\n",
    "# Nur auf Zeilen mit gültigen Koordinaten prüfen\n",
    "valid_mask = buildings['coord_valid'].fillna(False)\n",
    "inside_series = pd.Series(False, index=buildings.index)\n",
    "inside_series.loc[valid_mask] = coords.loc[valid_mask].apply(\n",
    "    lambda r: hesse_nassau_polygon.contains(Point(float(r['locationLng']), float(r['locationLat']))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "buildings['in_hesse_nassau_geo'] = inside_series\n",
    "print(f\"Geo-Shape (Hessen-Nassau): Treffer {buildings['in_hesse_nassau_geo'].sum()} von {len(buildings)}\")\n",
    "buildings.loc[buildings['in_hesse_nassau_geo']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3269c",
   "metadata": {},
   "source": [
    "## 4. Normalisierung mehrfacher Rollen-/Personenfelder\n",
    "\n",
    "Viele Spalten enden auf Sequenzen `_1`…`_10` (z. B. `ARCHITECTS_1`, `ARCHITECTS_2`, ...). Werteform: `uuid|Nachname, Vorname`.\n",
    "\n",
    "Ziel: Long-Format-Relation `building_person_roles`:\n",
    "(`building_id`, `role` (z. B. `ARCHITECTS`), `sequence` (Nummer), `person_id` (UUID), `person_label`).\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "1. Identifiziere alle Basis-Rollen (Teil vor letztem `_` + Ziffern).\n",
    "2. Iteriere über alle diese Spalten, extrahiere Werte ≠ leer.\n",
    "3. Teile an erster Pipe `|` → `person_id`, zweiter Teil `person_label` (Fallback: kompletter String falls kein `|`).\n",
    "4. Entferne potenzielle Dubletten (gleiche Kombination building_id + role + person_id).\n",
    "5. Erzeuge optionale Personentabelle `persons` (distinct `person_id`, `person_label`, `label_clean`).\n",
    "6. Bereinige `person_label`: Whitespace trimmen, vereinheitliche Kommaspacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce84ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_cols = [c for c in buildings.columns if re.search(r'_\\d+$', c)]\n",
    "base_roles = sorted(set(re.sub(r'_\\d+$','', c) for c in role_cols))\n",
    "rows = []\n",
    "for role in base_roles:\n",
    "    for i in range(1, 11):\n",
    "        col = f'{role}_{i}'\n",
    "        if col not in buildings.columns: \n",
    "            continue\n",
    "        for _, r in buildings[['ID', col]].iterrows():\n",
    "            val = r[col]\n",
    "            if pd.isna(val) or not str(val).strip():\n",
    "                continue\n",
    "            if '|' in val:\n",
    "                pid, label = val.split('|',1)\n",
    "            else:\n",
    "                pid, label = None, val\n",
    "            rows.append((r.ID, role, i, pid, label.strip()))\n",
    "\n",
    "building_person_roles = pd.DataFrame(rows, columns=['building_id','role','sequence','person_id','person_label'])\n",
    "persons = (building_person_roles\n",
    "           .dropna(subset=['person_label'])\n",
    "           .groupby(['person_id','person_label'], dropna=False)\n",
    "           .size().reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac288ef",
   "metadata": {},
   "source": [
    "Validierung:\n",
    "\n",
    "- Wie viele Rollen wurden extrahiert?\n",
    "- Top 3 Personen je Rolle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814fd55",
   "metadata": {},
   "source": [
    "## 5. Qualitätsmetriken & Checks\n",
    "\n",
    "Erzeuge kleine Metriken (als DataFrame oder Markdown):\n",
    "\n",
    "- Parsebarkeit Datum (%).\n",
    "- Anzahl unparsed Datumseinträge.\n",
    "- Anzahl extrahierter Personenbeziehungen.\n",
    "- #Distinct Personen.\n",
    "- Anteil gültiger Koordinaten.\n",
    "\n",
    "## OpenRefine\n",
    "\n",
    "Ziel: Schnelle, reproduzierbare Bereinigung & Normalisierung zentraler Felder aus `buildings.csv` mittels OpenRefine.\n",
    "\n",
    "> Fokus: Sicht auf typische OpenRefine-Operationen (Facets, Clustering, Transformations, Splits, Rekonsilierungsvorbereitung). Dauer: ca. 30–40 Minuten.\n",
    "\n",
    "### 1. Import\n",
    "\n",
    "- Lade `buildings.csv` in OpenRefine.\n",
    "- Alle Spalten zunächst als Text lassen (kein Auto-Parsing von Zahlen/Datumsangaben aktivieren).\n",
    "- Projektname: `buildings_raw`.\n",
    "\n",
    "### 2. Grundlegende Sichtbarkeit\n",
    "\n",
    "- Entferne (nur in der Ansicht, nicht dauerhaft) extrem leere Spalten via Facet → „Facet by blank“ und spätere Auswahl für Export.\n",
    "- Erzeuge eine Text-Facet auf `appellation` → erkenne Varianten / Dubletten.\n",
    "\n",
    "### 3. Bereinigung `verbaleDating`\n",
    "\n",
    "Ziel: Segmentierung & Vor-Normalisierung für spätere Python-Verarbeitung.\n",
    "\n",
    "Schritte:\n",
    "\n",
    "1. Expression (GREL) Trim: `value.trim()` (Spalte bearbeiten → Zellen transformieren).\n",
    "2. Ersetze mehrere Leerzeichen: `value.replace(/\\s+/,' ')`.\n",
    "3. Split bei Komma (Spaltenmenü → „Edit cells → Split multi-valued cells“ → Separator `,`).\n",
    "4. Neue Spalte aus dieser (JSON-Serialisierung einzelner Werte für Kontrolle): `value` (Beibehalten). Optional: Werte mit Regex-Facet `^[0-9]{3,4}(-[0-9]{3,4})?$` filtern (parsbar vs. unparsed).\n",
    "5. Erzeuge Booleanspalte `is_range`: GREL: `value.match(/^[0-9]{3,4}-[0-9]{3,4}$/) != null`.\n",
    "6. Erzeuge Spalten `year_start` und `year_end`:\n",
    "   - Falls Range: `value.match(/^(\\d{3,4})-(\\d{3,4})$/)[0]` & `[1]`\n",
    "   - Falls Einzeljahr: `value` in beide kopieren.\n",
    "7. Filtere Ausreißer: Facet `year_start` → Numeric Facet → Werte außerhalb 800–(aktuelles Jahr) markieren.\n",
    "\n",
    "Hinweis: Exportiere Zwischenergebnis als `building_dates_openrefine.csv` für Abgleich mit Pandas-Parsing.\n",
    "\n",
    "### 4. Personen-/Rollenfelder (Beispiel: ARCHITECTS)\n",
    "\n",
    "Ziel: Long-Format Grundlage.\n",
    "\n",
    "Schritte:\n",
    "\n",
    "1. Wähle Spalten `ARCHITECTS_1` … `ARCHITECTS_10`.\n",
    "2. „Edit columns → Join columns“ (Separator `||`), erzeuge Sammelspalte `ARCHITECTS_JOIN`.\n",
    "3. Split multi-valued cells an `||` → leere entfernen.\n",
    "4. Entferne Duplikate (Facet by text, Auswahl blank → ausschließen).\n",
    "5. Extrahiere UUID & Label:\n",
    "   - Neue Spalte `person_id`: GREL: `if(value.contains('|'), value.split('|')[0], null)`\n",
    "   - Neue Spalte `person_label_raw`: GREL: `if(value.contains('|'), value.split('|')[1], value)`\n",
    "6. Clean Label: Neue Spalte `person_label`: `person_label_raw.trim().replace(/\\s*,\\s*/, ', ')`.\n",
    "7. Cluster (Edit cells → Cluster & edit) auf `person_label` (Method: key collision + metaphone3). Prüfe Zusammenführungen.\n",
    "8. Export als `architects_roles.csv` (Spalten: `building_row_index` / `person_id` / `person_label`). Optional: füge Quellspalte `ARCHITECTS` hinzu.\n",
    "\n",
    "### 5. Vorbereitung für Wikidata-Reconciliation\n",
    "\n",
    "Ziel: Spalte (z. B. `appellation` oder aufbereitete Personennamen) für spätere Abgleichung.\n",
    "\n",
    "Schritte:\n",
    "\n",
    "1. Entferne offensichtliche Zusätze (Regex Replace): Beispiel: `value.replace(/,\\s*Deutschland$/,'')` (nur wenn sinnvoll!).\n",
    "2. Normalisiere Großschreibung: `value.toTitlecase()` (sparsam einsetzen, um historische Schreibweisen nicht zu verfälschen).\n",
    "3. Exportiere Liste eindeutiger Werte (`Facet → Export → tabular`) als `appellations_unique.csv`.\n",
    "\n",
    "### 6. Audit & Undo/Redo\n",
    "\n",
    "- Nutze das Undo/Redo Panel, dokumentiere die angewandten Schritte (Screenshot / JSON). Export: „Extract…“ → Speichere Transformations-JSON als `openrefine-history.json` für Reproduzierbarkeit.\n",
    "\n",
    "### 7. Kurzer Qualitätsbericht (Markdown außerhalb OpenRefine)\n",
    "\n",
    "- Anzahl ursprünglicher vs. bereinigter `verbaleDating` Tokens.\n",
    "- Anzahl zusammengeführter Personenlabels durch Clustering.\n",
    "- Wichtige offene Problemfälle (Stichpunkte).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
