{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ea027b",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 – Daten-Preprocessing\n",
    "\n",
    "Ziel: Rohdaten aus `buildings.csv` explorieren, bereinigen, normalisieren und für weitere Schritte (Anreicherung/Visualisierung) vorbereiten.\n",
    "\n",
    "Hinweis: Bei der Verwendung der [Wikimedia PAWS Instanz](https://hub-paws.wmcloud.org/hub) bitte dieses Notebook und die zugehörigen Dateien separat hochladen (Upload-Symbol in der Seitenleiste). Das Projekt findet sich im [GitHub-Repository](https://github.com/kristbaum/historical-data-hacking).\n",
    "\n",
    "## Installation von Abhängigkeiten\n",
    "\n",
    "* [Pandas](https://pandas.pydata.org/) – Datenanalyse-Toolkit für tabellarische Daten (DataFrames) zum Einlesen, Filtern und Transformieren.\n",
    "* [NumPy](https://numpy.org/) – Numerische Grundbibliothek für effiziente Arrays sowie Vektor- und Matrixrechnungen.\n",
    "* [Shapely](https://shapely.readthedocs.io/en/stable/) – Geometrieoperationen (Points, Polygons) und räumliche Analysen, z. B. Punkt-in-Polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b67db3-48a4-4d99-a102-88506cc0abdd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages into this notebook's Python kernel\n",
    "%pip install -q --upgrade pip\n",
    "%pip install -q pandas numpy shapely  # shapely for geospatial stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24d377-a3fa-4682-ba8f-82fed59cec54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Viele Spalten enthalten gemischte / freie Werte\n",
    "# → zunächst als Strings importieren vermeidez fehlerhafte automatische Typkonvertierungen.\n",
    "buildings = pd.read_csv('buildings.csv', dtype=str)\n",
    "print(buildings.shape)\n",
    "\n",
    "# Zeigt einen Ausschnitt der ersten 5 Zeilen der Tabelle\n",
    "buildings.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b0bb3-877b-440d-8196-445904f7f175",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Erste Profilierung\n",
    "\n",
    "1. Zähle Null-/Leerwerte pro Spalte (leerer String `''` zählt als leer).\n",
    "2. Ermittle den Anteil gefüllter Werte je Spalte.\n",
    "3. Erzeuge ein [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) mit: Spaltenname, Non-Null-%, Anzahl unterschiedlicher Werte.\n",
    "4. Entferne leere bzw. nahezu leere Spalten, um das Datenset übersichtlicher zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54306dd-f123-442c-ba03-c683ed24883c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1.\n",
    "# Ersetze Leerstrings und Spaces durch NA (fehlende Werte)\n",
    "data_with_normalized_nulls = buildings.replace('', np.nan)\n",
    "\n",
    "# Liste für Spaltenprofile erstellen\n",
    "profile_list = []\n",
    "\n",
    "# Iteriere über alle Spalten und sammle Statistiken\n",
    "for column_name in data_with_normalized_nulls.columns:\n",
    "    column_series = data_with_normalized_nulls[column_name]\n",
    "    \n",
    "    # Berechne Anteil nicht-Null Werte (in Prozent)\n",
    "    non_null_percentage = column_series.notna().mean() * 100\n",
    "    \n",
    "    # Zähle einzigartige Werte (ohne NA)\n",
    "    unique_value_count = column_series.nunique(dropna=True)\n",
    "    \n",
    "    \n",
    "    # Füge Profil zur Liste hinzu\n",
    "    profile_list.append({\n",
    "        'column': column_name,\n",
    "        'non_null_pct': non_null_percentage,\n",
    "        'n_unique': unique_value_count,\n",
    "    })\n",
    "\n",
    "# Erstelle DataFrame aus den Profilen und sortiere nach Anteil nicht-Null Werte\n",
    "print(pd.DataFrame(profile_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Entferne Spalten mit sehr wenigen/nicht vorhandenen Werten\n",
    "# Schwelle in Prozent (anpassbar): Spalten mit weniger als diesem Anteil an nicht-leeren Werten werden entfernt\n",
    "min_non_null_pct = 5.0  # z. B. 5%\n",
    "\n",
    "# Behandle '' und ' ' als fehlend für die Berechnung (ohne Side-Effects auf buildings)\n",
    "normalized = buildings.replace({'': pd.NA, ' ': pd.NA}).infer_objects(copy=False)\n",
    "non_null_pct = normalized.notna().mean() * 100\n",
    "\n",
    "# Optional: wichtige Spalten schützen, damit sie nicht versehentlich entfernt werden\n",
    "protected_cols = [c for c in ['ID', 'appellation', 'verbaleDating', 'locationLat', 'locationLng'] if c in buildings.columns]\n",
    "\n",
    "# Bestimme zu entfernende Spalten\n",
    "_to_drop = non_null_pct[non_null_pct < min_non_null_pct].index.tolist()\n",
    "# geschützte Spalten nicht droppen\n",
    "_to_drop = [c for c in _to_drop if c not in protected_cols]\n",
    "\n",
    "# Entferne Spalten in-place\n",
    "before_cols = buildings.shape[1]\n",
    "buildings.drop(columns=_to_drop, inplace=True, errors='ignore')\n",
    "after_cols = buildings.shape[1]\n",
    "\n",
    "print(f\"Entfernte Spalten (non-null < {min_non_null_pct}%): {len(_to_drop)} | Vorher: {before_cols}, Nachher: {after_cols}\")\n",
    "\n",
    "\n",
    "# Optional: zeige die 10 leersten verbleibenden Spalten zur Kontrolle\n",
    "print('\\nLeerste verbleibende Spalten (Top 10):')\n",
    "print(non_null_pct.loc[buildings.columns].sort_values().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4defa",
   "metadata": {},
   "source": [
    "Fragen:\n",
    "\n",
    "- Welche Spalten sind fast leer und sollten ggf. ausgelagert oder ignoriert werden?\n",
    "- Gibt es offensichtliche Duplikate bei `ID`?\n",
    "\n",
    "## 2. Bereinigung `verbaleDating`\n",
    "\n",
    "Die Spalte enthält verbale Zeitangaben/Bereiche, z. B.: `\"1000-2020, 1773-1776\"`.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "1. Zerlege `verbaleDating` in einzelne Segmente (Trennzeichen: Komma) und normalisiere Leerzeichen.\n",
    "2. Identifiziere Bereiche (Pattern `YYYY-YYYY`) vs. Einzeljahre (`YYYY`).\n",
    "3. Baue eine normalisierte Tabelle `building_dates`: (`building_id`, `date_raw`, `year_start`, `year_end`, `is_range`, `precision`).\n",
    "4. Berechne je Gebäude: minimaler Start und maximaler Endwert → `chronology_min`, `chronology_max`, und füge sie dem Haupt-DataFrame wieder hinzu.\n",
    "5. Detektiere Ausreißer (z. B. Jahr < 1000 oder > aktuelles Jahr) und markiere sie für manuelle Prüfung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1345e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# - Wir suchen die ersten 1–2 vierstelligen Jahreszahlen (\\bYYYY\\b) in der\n",
    "#   verbal_Dating SPalte in der Reihenfolge ihres Auftretens.\n",
    "# - Wenn nur ein Jahr gefunden wird, setzen wir year_start = year_end = dieses Jahr.\n",
    "# - Wenn zwei Jahre gefunden werden, verwenden wir das erste als year_start und das\n",
    "#   zweite als year_end.\n",
    "\n",
    "def _parse_first_two_years(s):\n",
    "    if not isinstance(s, str):\n",
    "        return (pd.NA, pd.NA)\n",
    "    matches = re.findall(r\"\\b(\\d{4})\\b\", s)\n",
    "    if not matches:\n",
    "        return (pd.NA, pd.NA)\n",
    "    y1 = int(matches[0])\n",
    "    y2 = int(matches[1]) if len(matches) > 1 else y1\n",
    "    return (y1, y2)\n",
    "\n",
    "# Wende parser auf jede Zeile an\n",
    "years = buildings['verbaleDating'].fillna('').map(_parse_first_two_years)\n",
    "# Zerlege die Tupel in eine DataFrame mit gleichen Indices\n",
    "year_df = pd.DataFrame(years.tolist(), index=buildings.index, columns=['year_start','year_end'])\n",
    "\n",
    "# Schreibe zurück in 'buildings' als Zahl\n",
    "buildings['year_start'] = year_df['year_start'].astype('Int64')\n",
    "buildings['year_end']   = year_df['year_end'].astype('Int64')\n",
    "\n",
    "# Einfache Kennzahlen und Beispiele\n",
    "vd_raw = buildings['verbaleDating'].fillna('')\n",
    "rows_with_text = int(vd_raw.str.strip().ne('').sum())\n",
    "rows_with_parsed = int(buildings[['year_start','year_end']].notna().any(axis=1).sum())\n",
    "\n",
    "print(f\"Nicht leere verbal_Dating Zeilen: {rows_with_text}\")\n",
    "print(f\"Zeilen mit zugeordneten Jahr(en): {rows_with_parsed} von {len(buildings)}\")\n",
    "\n",
    "# Beispiele: Vorher -> Nachher (year_start/year_end)\n",
    "examples = buildings.loc[vd_raw.str.strip().ne(''), ['verbaleDating','year_start','year_end']].head(10)\n",
    "print(\"\\nBeispiele (Vorher: verbaleDating → Nachher: year_start/year_end):\")\n",
    "print(examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac69222",
   "metadata": {},
   "source": [
    "## 3. Geodaten-Auswertung\n",
    "\n",
    "1. Konvertiere `locationLat`/`locationLng` zu Fließkommazahlen.\n",
    "2. Prüfe Wertebereiche der Koordinaten (alle sollten ca. in DE liegen; [Eckpunkte siehe Wikidata](https://www.wikidata.org/wiki/Q183)).\n",
    "3. Finde alle Gebäude im heutigen Hessen per Bounding Box (gedachtes Rechteck; grob von einer Karte geschätzt).\n",
    "4. Beschränke auf die historische [Provinz Hessen-Nassau](https://de.wikipedia.org/wiki/Provinz_Hessen-Nassau) via Geoshape. Daten von [OpenHistoricalMap](https://www.openhistoricalmap.org/relation/2690442), Abfrage via [Overpass](https://overpass-turbo.openhistoricalmap.org/?Q=%5Bout%3Ajson%5D%5Btimeout%3A120%5D%3B%0Arel%282690442%29%3B%0A%28._%3B%3E%3B%29%3B%0Aout%20body%20geom%3B&C=50.004209%3B11.518449%3B7) (Export → Geoshape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05462199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Konvertierung zu Fließkommazahlen\n",
    "def to_float(s):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except (TypeError, ValueError):\n",
    "        return pd.NA\n",
    "\n",
    "coords = buildings[['locationLat','locationLng']].map(to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc927dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Wertebereich in ca. DE\n",
    "valid_lat = coords.locationLat.between(47,55)\n",
    "valid_lng = coords.locationLng.between(5,16)\n",
    "buildings['coord_valid'] = valid_lat & valid_lng\n",
    "\n",
    "\n",
    "# Liste invalide Locations\n",
    "invalid_mask = ~buildings['coord_valid'].fillna(False)\n",
    "if invalid_mask.any():\n",
    "    print(buildings.loc[invalid_mask, ['ID','appellation','locationLat','locationLng']])\n",
    "else:\n",
    "    print(\"Keine ungültigen Koordinaten gefunden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Bounding Box für Hessen\n",
    "# Bounding Box ungefähr für Hessen (Nordwest / Südost):\n",
    "NW_LAT, NW_LNG = 51.599, 8.072   # Nordwestlichster Punkt\n",
    "SE_LAT, SE_LNG = 49.495, 10.088  # Südöstlichster Punkt\n",
    "\n",
    "\n",
    "# Min/Max für Bereichsprüfung bestimmen\n",
    "min_lat, max_lat = SE_LAT, NW_LAT\n",
    "min_lng, max_lng = NW_LNG, SE_LNG\n",
    "\n",
    "# Masken für BBox prüfen (nur valide Koordinaten berücksichtigen)\n",
    "in_bbox = (\n",
    "    coords.locationLat.between(min_lat, max_lat)\n",
    "    & coords.locationLng.between(min_lng, max_lng)\n",
    ")\n",
    "mask_bbox = in_bbox & buildings['coord_valid'].fillna(False)\n",
    "\n",
    "# Ergebnis: Gefilterte Entitäten in Hessen (via BBox)\n",
    "buildings['in_hesse_bbox'] = mask_bbox\n",
    "buildings_hesse_bbox = buildings.loc[mask_bbox].copy()\n",
    "\n",
    "print(\n",
    "    f\"BBox Hessen: lat [{min_lat}, {max_lat}], lng [{min_lng}, {max_lng}] | \"\n",
    "    f\"Treffer: {mask_bbox.sum()} von {len(buildings)}\"\n",
    ")\n",
    "buildings_hesse_bbox.head(10)\n",
    "\n",
    "# Wie man an addressState ganz gut erkennen kann, sind auch ein paar Falsche mit dabei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad614f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Punkt-in-Polygon: Historisches Hessen-Nassau\n",
    "import json\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "# GeoJSON laden (Datei im aktuellen Ordner erwartet)\n",
    "with open('./hessen_nassau.geojson', 'r', encoding='utf-8') as f:\n",
    "    gj = json.load(f)\n",
    "\n",
    "# Geometrie aus FeatureCollection/Feature/Geometry entnehmen\n",
    "geom = (\n",
    "    gj['features'][0]['geometry'] if gj.get('type') == 'FeatureCollection'\n",
    "    else (gj['geometry'] if gj.get('type') == 'Feature' else gj)\n",
    ")\n",
    "hesse_nassau_polygon = shape(geom)\n",
    "\n",
    "# Nur auf Zeilen mit gültigen Koordinaten prüfen\n",
    "valid_mask = buildings['coord_valid'].fillna(False)\n",
    "inside_series = pd.Series(False, index=buildings.index)\n",
    "inside_series.loc[valid_mask] = coords.loc[valid_mask].apply(\n",
    "    lambda r: hesse_nassau_polygon.contains(Point(float(r['locationLng']), float(r['locationLat']))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "buildings['in_hesse_nassau_geo'] = inside_series\n",
    "print(f\"Geo-Shape (Hessen-Nassau): Treffer {buildings['in_hesse_nassau_geo'].sum()} von {len(buildings)}\")\n",
    "buildings.loc[buildings['in_hesse_nassau_geo']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export current 'buildings' DataFrame for OpenRefine\n",
    "out_path = './buildings_openrefine.csv'\n",
    "buildings.to_csv(out_path, index=False, encoding='utf-8')\n",
    "print(f\"Exported buildings -> {out_path} ({buildings.shape[0]} rows, {buildings.shape[1]} cols)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814fd55",
   "metadata": {},
   "source": [
    "## OpenRefine\n",
    "\n",
    "Bereinigung und Normalisierung zentraler Felder aus `buildings_openrefine.csv` mit OpenRefine. Viele der Pandas-Schritte wären in OpenRefine sehr aufwändig; daher arbeiten wir auf dem Export der Vorverarbeitung weiter.\n",
    "\n",
    "### 1. Import `buildings_openrefine.csv`\n",
    "\n",
    "- Lade `buildings_openrefine.csv` in OpenRefine (Create Project → File from this Computer).\n",
    "- Belasse alle Spalten zunächst als Text (Option „Attempt to parse cell text into numbers“ deaktiviert lassen).\n",
    "\n",
    "### 2. Grundlegende Bereinigungen\n",
    "\n",
    "- Spalte `appellation`: Edit cells → Common transforms → Trim leading and trailing whitespace und ggf. Collapse consecutive whitespace.\n",
    "- Erzeuge eine Duplikat-Facette auf `appellation` (Facets → Customized facets → Facet by duplicates). Markiere doppelte Zeilen per Flag, filtere unter All → Facet by flag → true und entferne sie über All → Edit rows → Remove matching rows.\n",
    "\n",
    "### 4. `people.csv` bereinigen\n",
    "\n",
    "- `people.csv` ins Projekt laden.\n",
    "- Überflüssige Spalten entfernen:\n",
    "  * `verbaleDating` ist leer: prüfen über Spaltenmenü → Facet → Text facet; anschließend löschen via Edit column → Remove this column.\n",
    "  * Gleiches mit der Spalte `resource_urls` durchführen.\n",
    "\n",
    "### 5. Clustering zur Dublettenerkennung\n",
    "\n",
    "[Clustering](https://openrefine.org/docs/technical-reference/clustering-in-depth) ist ein fortgeschrittener Ansatz zur Erkennung von Dubletten (Tippfehler, alternative Schreibweisen).\n",
    "\n",
    "- Spalte `appellation`: Edit cells → Cluster and edit.\n",
    "- Im Clustering-Dialog zuerst Methode „Key collision“ mit Keying function „Fingerprint“ ausführen (Cluster). Diese Methode findet u. a. Wortdreher/falsch positionierte Buchstaben. Prüfen, ob echte Duplikate vorliegen.\n",
    "- Danach Methode „Nearest neighbour“ mit Distanzfunktion „Levenshtein“ (erneut Cluster). Größerer Radius findet mehr Unterschiede, produziert aber auch mehr False Positives. In beiden Fällen gibt es keine echten Duplikate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
